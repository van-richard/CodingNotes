{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9216444-cd2b-48e4-bb0b-6fdb7051716f",
   "metadata": {},
   "source": [
    "# Slurm\n",
    "\n",
    "## Overview\n",
    "\n",
    "SLURM is an open source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small Linux clusters. \n",
    "\n",
    "[slurm.schemd.com](https://slurm.schedmd.com/overview.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c293efa8-3587-47e3-b99e-eb4c540b8943",
   "metadata": {},
   "source": [
    "1. Job Submission/Management\n",
    "2. Resource Allocation/Utilization\n",
    "3. Scripting and Automation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2126e7f0-7739-4c5b-88f1-ab95f9aca2c3",
   "metadata": {},
   "source": [
    "## Job Sumission\n",
    "\n",
    "### Directives (#SBATCH)\n",
    "\n",
    "The way that Slurm determines how to allocate your jobs to the cluster (i.e. across how many compute nodes, with how many CPUs, for how long etc) is via Slurm directives that are included at the top of your job script. These directives are indicated by lines starting with `#SBATCH`\n",
    "\n",
    "#### Example: CPU Job\n",
    "\n",
    "Change the words in all-caps to what you need. \n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "#SBATCH --partition=PARTITION\n",
    "#SBATCH --time=TIME (DAYS-HOURS:MINUTES:SECONDS)\n",
    "#SBATCH --nodes=NODES\n",
    "#SBATCH --ntasks=NTASKS\n",
    "#SBATCH --output=%j.out \n",
    "#SBATCH --error=%j.err\n",
    "#SBATCH --name=JOBNAME\n",
    "```\n",
    "\n",
    "#### Example: GPU Job\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "#SBATCH --partition=GPU_PARTITION\n",
    "#SBATCH --time=TIME (DAYS-HOURS:MINUTES:SECONDS)\n",
    "#SBATCH --nodes=NODES\n",
    "#SBATCH --ntasks=NTASKS\n",
    "#SBATCH --output=%j.out \n",
    "#SBATCH --error=%j.err\n",
    "#SBATCH --name=JOBNAME\n",
    "#SBATCH --gres=gpu:1\n",
    "```\n",
    "\n",
    "#### Example: Exclusive Job\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "#SBATCH --partition=GPU_PARTITION\n",
    "#SBATCH --time=TIME (DAYS-HOURS:MINUTES:SECONDS)\n",
    "#SBATCH --nodes=NODES\n",
    "#SBATCH --ntasks=NTASKS\n",
    "#SBATCH --output=%j.out \n",
    "#SBATCH --error=%j.err\n",
    "#SBATCH --name=JOBNAME\n",
    "#SBATCH --mem=0\n",
    "#SBATCH --exclusive\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce6c29b-7dcc-4dbf-bc4b-8a1c564b4b5e",
   "metadata": {},
   "source": [
    "## Job Managment\n",
    "\n",
    "Submitting a job\n",
    "\n",
    "```bash\n",
    "sbatch job.slurm\n",
    "```\n",
    "\n",
    "Canceling a job\n",
    "\n",
    "```bash\n",
    "scancel [JOB ID]\n",
    "```\n",
    "\n",
    "Show queued/running jobs which can be all queued/running jobs or just your queued/jobs\n",
    "\n",
    "```bash\n",
    "squeue # All jobs queue\n",
    "squeue -u *USERNAME* # USERNAME queue\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ab937d-b062-49df-900e-2aea887cd76b",
   "metadata": {},
   "source": [
    "## Resource Allocation\n",
    "\n",
    "Get information about the resources on available nodes that make up the HPC cluster \n",
    "\n",
    "```bash \n",
    "sinfo # All resources \n",
    "sinfo | grep idle # Idle nodes \n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7042f979-b3c5-4d16-8baf-05d541375066",
   "metadata": {},
   "source": [
    "## Resource Management\n",
    "\n",
    "*Explain Slurm directives and only requesting needed resources*\n",
    "\n",
    "Additionally, the HPC often has preinstalled software. You can find this using the `module` command. The `module` system to load most software into a userâ€™s environment. Some software may not be accessible by default and must be loaded in. This allows Research Computing to provide multiple versions of the software concurrently and enables users to easily switch between different versions.\n",
    "\n",
    "### Example commands\n",
    "\n",
    "List all available modules\n",
    "\n",
    "```bash\n",
    "module av\n",
    "```\n",
    "\n",
    "Load a module\n",
    "\n",
    "```bash\n",
    "module load *name*\n",
    "```\n",
    "\n",
    "Remove a module \n",
    "\n",
    "```bash\n",
    "module remove *name*\n",
    "```\n",
    "\n",
    "Remove ALL modules, cleaning your environment\n",
    "\n",
    "```bash\n",
    "module purge\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6790f227-44c7-45d4-b4b8-8cab79723899",
   "metadata": {},
   "source": [
    "## Scripting "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625c2282-d205-4822-b19a-ab8fa119beba",
   "metadata": {},
   "source": [
    "## Automation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e9140d-b43c-42d1-a450-56b9ef046bbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
